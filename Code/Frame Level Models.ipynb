{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code has been adapted from starter code provided by Google to submit jobs\n",
    "# on Google cloud and train the models\n",
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "\n",
    "\n",
    "\"\"\"Contains a collection of models which operate on variable-length sequences.\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import models\n",
    "import video_level_models\n",
    "import tensorflow as tf\n",
    "import model_utils as utils\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow import flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\"iterations\", 30,\n",
    "                     \"Number of frames per batch for DBoF.\")\n",
    "flags.DEFINE_bool(\"dbof_add_batch_norm\", True,\n",
    "                  \"Adds batch normalization to the DBoF model.\")\n",
    "flags.DEFINE_bool(\n",
    "    \"sample_random_frames\", True,\n",
    "    \"If true samples random frames (for frame level models). If false, a random\"\n",
    "    \"sequence of frames is sampled instead.\")\n",
    "flags.DEFINE_integer(\"dbof_cluster_size\", 8192,\n",
    "                     \"Number of units in the DBoF cluster layer.\")\n",
    "flags.DEFINE_integer(\"dbof_hidden_size\", 1024,\n",
    "                     \"Number of units in the DBoF hidden layer.\")\n",
    "flags.DEFINE_string(\"dbof_pooling_method\", \"max\",\n",
    "                    \"The pooling method used in the DBoF cluster layer. \"\n",
    "                    \"Choices are 'average' and 'max'.\")\n",
    "flags.DEFINE_string(\"video_level_classifier_model\", \"MoeModel\",\n",
    "                    \"Some Frame-Level models can be decomposed into a \"\n",
    "                    \"generalized pooling operation followed by a \"\n",
    "                    \"classifier layer\")\n",
    "flags.DEFINE_integer(\"lstm_cells\", 1024, \"Number of LSTM cells.\")\n",
    "flags.DEFINE_integer(\"lstm_layers\", 2, \"Number of LSTM layers.\")\n",
    "\n",
    "class FrameLevelLogisticModel(models.BaseModel):\n",
    "\n",
    "  def create_model(self, model_input, vocab_size, num_frames, **unused_params):\n",
    "    \"\"\"Creates a model which uses a logistic classifier over the average of the\n",
    "    frame-level features.\n",
    "    This class is intended to be an example for implementors of frame level\n",
    "    models. If you want to train a model over averaged features it is more\n",
    "    efficient to average them beforehand rather than on the fly.\n",
    "    Args:\n",
    "      model_input: A 'batch_size' x 'max_frames' x 'num_features' matrix of\n",
    "                   input features.\n",
    "      vocab_size: The number of classes in the dataset.\n",
    "      num_frames: A vector of length 'batch' which indicates the number of\n",
    "           frames for each video (before padding).\n",
    "    Returns:\n",
    "      A dictionary with a tensor containing the probability predictions of the\n",
    "      model in the 'predictions' key. The dimensions of the tensor are\n",
    "      'batch_size' x 'num_classes'.\n",
    "    \"\"\"\n",
    "    num_frames = tf.cast(tf.expand_dims(num_frames, 1), tf.float32)\n",
    "    feature_size = model_input.get_shape().as_list()[2]\n",
    "\n",
    "    denominators = tf.reshape(\n",
    "        tf.tile(num_frames, [1, feature_size]), [-1, feature_size])\n",
    "    avg_pooled = tf.reduce_sum(model_input,\n",
    "                               axis=[1]) / denominators\n",
    "\n",
    "    output = slim.fully_connected(\n",
    "        avg_pooled, vocab_size, activation_fn=tf.nn.sigmoid,\n",
    "        weights_regularizer=slim.l2_regularizer(1e-8))\n",
    "    return {\"predictions\": output}\n",
    "\n",
    "\n",
    "\n",
    "class LstmModel(models.BaseModel):\n",
    "\n",
    "  def create_model(self, model_input, vocab_size, num_frames, **unused_params):\n",
    "    \"\"\"Creates a model which uses a stack of LSTMs to represent the video.\n",
    "    Args:\n",
    "      model_input: A 'batch_size' x 'max_frames' x 'num_features' matrix of\n",
    "                   input features.\n",
    "      vocab_size: The number of classes in the dataset.\n",
    "      num_frames: A vector of length 'batch' which indicates the number of\n",
    "           frames for each video (before padding).\n",
    "    Returns:\n",
    "      A dictionary with a tensor containing the probability predictions of the\n",
    "      model in the 'predictions' key. The dimensions of the tensor are\n",
    "      'batch_size' x 'num_classes'.\n",
    "    \"\"\"\n",
    "    lstm_size = FLAGS.lstm_cells\n",
    "    number_of_layers = FLAGS.lstm_layers\n",
    "\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "            [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    lstm_size, forget_bias=1.0)\n",
    "                for _ in range(number_of_layers)\n",
    "                ])\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_lstm, model_input,\n",
    "                                       sequence_length=num_frames,\n",
    "                                       dtype=tf.float32)\n",
    "\n",
    "    aggregated_model = getattr(video_level_models,\n",
    "                               FLAGS.video_level_classifier_model)\n",
    "\n",
    "    return aggregated_model().create_model(\n",
    "        model_input=state[-1].h,\n",
    "        vocab_size=vocab_size,\n",
    "        **unused_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
