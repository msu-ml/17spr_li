{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code has been adapted from starter code provided by Google to submit jobs\n",
    "# on Google cloud and train the models\n",
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "\n",
    "\n",
    "\"\"\"Contains model definitions.\"\"\"\n",
    "import math\n",
    "\n",
    "import models\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from tensorflow import flags\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\n",
    "    \"moe_num_mixtures\", 2,\n",
    "    \"The number of mixtures (excluding the dummy 'expert') used for MoeModel.\")\n",
    "\n",
    "class LogisticModel(models.BaseModel):\n",
    "  \"\"\"Logistic model with L2 regularization.\"\"\"\n",
    "\n",
    "  def create_model(self, model_input, vocab_size, l2_penalty=1e-8, **unused_params):\n",
    "    \"\"\"Creates a logistic model.\n",
    "    Args:\n",
    "      model_input: 'batch' x 'num_features' matrix of input features.\n",
    "      vocab_size: The number of classes in the dataset.\n",
    "    Returns:\n",
    "      A dictionary with a tensor containing the probability predictions of the\n",
    "      model in the 'predictions' key. The dimensions of the tensor are\n",
    "      batch_size x num_classes.\"\"\"\n",
    "    output = slim.fully_connected(\n",
    "        model_input, vocab_size, activation_fn=tf.nn.sigmoid,\n",
    "        weights_regularizer=slim.l2_regularizer(l2_penalty))\n",
    "    return {\"predictions\": output}\n",
    "\n",
    "class MoeModel(models.BaseModel):\n",
    "  \"\"\"A softmax over a mixture of logistic models (with L2 regularization).\"\"\"\n",
    "\n",
    "  def create_model(self,\n",
    "                   model_input,\n",
    "                   vocab_size,\n",
    "                   num_mixtures=None,\n",
    "                   l2_penalty=1e-8,\n",
    "                   **unused_params):\n",
    "    \"\"\"Creates a Mixture of (Logistic) Experts model.\n",
    "     The model consists of a per-class softmax distribution over a\n",
    "     configurable number of logistic classifiers. One of the classifiers in the\n",
    "     mixture is not trained, and always predicts 0.\n",
    "    Args:\n",
    "      model_input: 'batch_size' x 'num_features' matrix of input features.\n",
    "      vocab_size: The number of classes in the dataset.\n",
    "      num_mixtures: The number of mixtures (excluding a dummy 'expert' that\n",
    "        always predicts the non-existence of an entity).\n",
    "      l2_penalty: How much to penalize the squared magnitudes of parameter\n",
    "        values.\n",
    "    Returns:\n",
    "      A dictionary with a tensor containing the probability predictions of the\n",
    "      model in the 'predictions' key. The dimensions of the tensor are\n",
    "      batch_size x num_classes.\n",
    "    \"\"\"\n",
    "    num_mixtures = num_mixtures or FLAGS.moe_num_mixtures\n",
    "\n",
    "    gate_activations = slim.fully_connected(\n",
    "        model_input,\n",
    "        vocab_size * (num_mixtures + 1),\n",
    "        activation_fn=None,\n",
    "        biases_initializer=None,\n",
    "        weights_regularizer=slim.l2_regularizer(l2_penalty),\n",
    "        scope=\"gates\")\n",
    "    expert_activations = slim.fully_connected(\n",
    "        model_input,\n",
    "        vocab_size * num_mixtures,\n",
    "        activation_fn=None,\n",
    "        weights_regularizer=slim.l2_regularizer(l2_penalty),\n",
    "        scope=\"experts\")\n",
    "\n",
    "    gating_distribution = tf.nn.softmax(tf.reshape(\n",
    "        gate_activations,\n",
    "        [-1, num_mixtures + 1]))  # (Batch * #Labels) x (num_mixtures + 1)\n",
    "    expert_distribution = tf.nn.sigmoid(tf.reshape(\n",
    "        expert_activations,\n",
    "        [-1, num_mixtures]))  # (Batch * #Labels) x num_mixtures\n",
    "\n",
    "    final_probabilities_by_class_and_batch = tf.reduce_sum(\n",
    "        gating_distribution[:, :num_mixtures] * expert_distribution, 1)\n",
    "    final_probabilities = tf.reshape(final_probabilities_by_class_and_batch,\n",
    "                                     [-1, vocab_size])\n",
    "    return {\"predictions\": final_probabilities}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
